---
title: "Latent variable analysis"
csl: the-journal-of-wildlife-management.csl
output:
  html_notebook: default
bibliography: bibliography.bib
editor_options: 
  chunk_output_type: inline
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message = FALSE}
library(tidyverse)
library(psych)

```

## Answers to the challenge


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message = FALSE}
library(tidyverse)
library(psych)

```

## Answers to the challenge

```{r}
library(tidyverse)
library(ggdendro)
library(NbClust)

data("USArrests")

USArrests %>% 
  scale() -> arrest.scale

arrest.scale %>% 
  dist(method = "euclidean") -> arrest.euc

arrest_hclust <- hclust(arrest.euc, method = "ward.D2")

### Find the best number of clusters
arrest.nb <- NbClust(arrest.scale, 
        distance = "euclidean", 
        min.nc = 2, 
        max.nc = 10, 
        method = "complete", 
        index = "all")

clusters <- data.frame(arrest.nb$Best.partition)
clusters$state <- rownames(clusters)
colnames(clusters) <- c("cluster", "label")

### extract the data
arrest_dendro <- as.dendrogram(arrest_hclust)

### convert data to dendro data for ggplot
arrest_dendro_data <- dendro_data(arrest_dendro, type = "rectangle")

### combine cluster and dendro label data 
arrest_dendro_labels <- data.frame(full_join(arrest_dendro_data$labels, clusters))

arrest_dendro_segs <- data.frame(arrest_dendro_data$segments) 
arrest_dendro_segs$cluster <- ifelse(arrest_dendro_segs$x < 20 & arrest_dendro_segs$xend < 20, 1, 2)

### plot 
arrest_dendro_plot<-ggplot() + 
                      geom_segment(data = arrest_dendro_segs, aes(x = x, y = y, xend = xend, yend = yend, color = as.factor(cluster))) + 
                      geom_text(data = arrest_dendro_labels, aes(x = x, y = y, label = label, color = as.factor(cluster)), angle = 90, hjust = 1, size = 1.7, show.legend = FALSE) +
                      labs(color="Cluster") +  
                      coord_cartesian(ylim = c(-5,15)) +
                      theme_void() + 
                      scale_colour_manual(values = c("1" = "firebrick", "2" = "dodgerblue")) +
                      theme(plot.margin = margin(0.5, 0.5, 1, 0.5, unit = "cm")
                      )


arrest_dendro_plot
```

## A quick word about eigenvectors and values

[Eigenvectors and Eigenvalues](http://setosa.io/ev/eigenvectors-and-eigenvalues/)

For any number set of points, we can digest these points into eigenvectors and  eigenvalue.  Eigenvectors and eigenvalues exist in pairs with eigenvector describes a direction along which a linear transformation acts simply by "stretching/compressing" and/or "flipping"; and eigenvalue describing the degree of that transformation in that direction.  The numbers of eigenvectors is equal to the number of dimensions of your data.  

```{r}
mat<-matrix(c(3,2,1,2), byrow = TRUE, ncol = 2)

eigen(mat)
```

# Latent variable analysis

## Factor analysis


The source to many of the notes in this lesson (and a lot more detail on the subject) can be found at @finch2015latent and @beaujean2014latent.  

*Latent variables* in statistics are variables that are not directly observable and are inferred from a mathematical model.  One advantage of using *latent variables* is that it helps reduce the dimensionality of data (a major theme of multivariate statistics) and has been used in many scientific disciplines.  

One type of latent variable analysis is *factor analysis* and used extensively in social and behavioral sciences.  Factor analysis allows the researcher to create models of non-observable factors (e.g., motivations, constraints, identity) from multivariate data.  

There are two broad types of factor analysis:  1) Exploratory factor analysis (EFA) and 2) Confirmatory factor analysis (CFA).  The difference between the two is in the degree of ** a priori ** structure that is assummed in the model.  In using EFA the researcher does not impose a specific latent structure on the observable data, but allows the analysis to provide the optimal number of factors. In contrast to EFA, with CFA the researcher explicitly links the indicators with the factors to which they theoretically belong.

## Exploratory factor analysis

EFA consists of two steps (1) factor extraction and (2) factor rotation.  Factor rotation involves estimating the intial model paramters (i.e., factor loadings:  loadings reflect the
relationships between the factors and the indicators, with larger values being indicative
of a closer association between a latent and observed variable).  There are as many factors as number of indicator variables (i.e., columns used to define the latent variable). 

### Factor extraction

Several extraction methods exist, with the most popular being maximum likelihood (ML) and principal axis factor (PAF).  ML method has a direct assessment of model fit but also relies on multivariate normality.  PAF does not have a distributional assumption but does not have a test of statistical fit.  

### Factor rotation

Factor rotation  is the transformation of the initial set of factor loadings to simplify the interpretation of of the results by finding a simple solution.  Methods fall into two broad categories:  orthogonal and oblique.  Orthogonal rotations constrain the correlations among factors to be zero, whereas oblique rotations allow the factors to be correlated. The most
popular orthogonal rotation method is VARIMAX, while among the oblique rotations PROMAX and OBLIMIN are popular.  Decision on which method to use, should be based in theory and empirical grounds.  

### Determining the optimal number of factors

1. Eigenvalues greater than 1.  Eigenvalues greater than 1, account for more variance than any of the observed values.  
2.  Scree plots.  Eigenvalues on the y-axis and number of factors on the x axis.  Identify where the plot "flattens out"
3. Proportion of variance.  Identify when there is no "appreciative"  gain in the percentage of variance explained.  Similar to scree plot
4. Residual correlation matrix for the observed indicators. residuals larger than 0.05 are considered to be too large, so that a good solution is one which produces few residual correlations greater than 0.05 in absolute value.
5. Parallel analysis.  Described in @horn1965rationale.  Several steps that include comparing the actual data to  two randomly generated datasets.  THis is the most commonly used. 

### The data 

For this data, we will use the data set provided by @finch2015latent [here](https://www.routledge.com/Latent-Variable-Modeling-with-R/Finch-French/p/book/9780415832458#datasets).  The data represents information collected on acheivement goal orientation. Achievement goal orientation refers to how an individual interprets and reacts to tasks, resulting in different patterns of cognition, affect and behavior.  There are 12 questions with results representing a 7-point likert-type scale from 430 college students. 

The columns refer to:

- AGS1 = My goal is to completely master the material presented in my classes. (MAP)
- AGS2 = I want to avoid learning less than it is possible to learn. (MAV)
- AGS3 = It is important for me to do better than other students. (PAP)
- AGS4 = I want to avoid performing poorly compared to others. (PAV)
- AGS5 = I want to learn as much as possible. (MAP)
- AGS6 = It is important for me to avoid an incomplete understanding of the course material. (MAV) 
- AGS7 = It is important for me to understand the content of my courses as thoroughly as possible. (MAP)
- AGS8 = My goal is to avoid performing worse than other students. (PAV) 
- AGS9 = I want to do well compared to other students. (PAP) 
- AGS10 = It is important for me to avoid doing poorly compared to other students. (PAV) 
- AGS11 = My goal is to perform better than the other students. (PAP) 
- AGS12 = My goal is to avoid learning less than I possibly could. (MAV)


The types of questions refer to 4 distinct latent traits: mastery approach (MAP), mastery avoidant (MAV), performance approach (PAP), and performance avoidant (PAV).

The data is in a SPSS format and I have converted it to a csv file for convience and is in our github data repository as `goal_scale.csv`

```{r}
library(readr)

goal_scale <- read_csv("https://raw.githubusercontent.com/chrischizinski/SNR_R_Group/master/data/goal_scale.csv")

head(goal_scale)

```


### Fit an EFA models with `factanal`

```{r}

agoal.efa<-factanal(~ags1+ags2+ags3+ags4+ags5+ags6+ags7+ags8+ags9+ags10+ags11+ ags12, factors=4, rotation="promax", data = goal_scale )

agoal.efa

```

#### Uniqueness 

Uniqueness reflects the proportion of variance in the indicators that are not explained by the factors.  For example, `r round(agoal.efa$uniquenesses[1], digits = 3)*100 `% of variation in `ags1` is not explained by the four factors. 


```{r}
agoal.efa$uniquenesses
```

The opposite of uniqueness of communality, and this is the proportion of variances explained by the factors for each indicator.

```{r}
1-agoal.efa$uniquenesses
```


#### Loadings

```{r}
ld<-loadings(agoal.efa)
ld

```
To help interpret our loadings, lets create a visualization of those loadings.  

```{r}
loadings<-as.data.frame(ld[,])

lt<- data.frame(indicator = paste("ags",1:12, sep =""),
           latent_traits = c("MAP", "MAV", "PAP", "PAV", "MAP","MAV", "MAP", "PAV", "PAP", "PAV", "PAP", "MAV"))

loadings %>% 
  rownames_to_column("indicator") %>% 
  left_join(lt) %>% 
  mutate(indicator = factor(indicator, levels = paste("ags",12:1, sep =""))) %>% 
  gather(factor, value, -indicator, - latent_traits) %>% 
  mutate(value2 = ifelse(value < 0.1, NA,  value))-> loadings.long 


ggplot(data = loadings.long) +
  geom_point(aes(x = factor, y = indicator, color = value2, shape = latent_traits), size = 8) +
  scale_colour_gradient(na.value = "white", low = "blue", high = "red") +
  scale_shape_manual(values = c("MAP" = 15, "MAV" = 16, "PAP" = 17, "PAV" = 18)) +
  labs(color = "Loading", shape = "Latent\ntrait") +
  theme_classic()
```

1.  We see some cross loading by indicators: `ags4`, `ags5`, `ags8`,`ags10`, `ags11`
2. We also see that each factor has multiple latent traits associated with it, suggesting this solution does not conform to the *a priori * 4-factor solution 

#### Variation explained by each factor

While this is printed using `loadings` it is not easily extracted using that function so we need to calculate it ourelves.  The sum of squared loadings (SS_loadings) is taken by multiplying the loadings by itself and then taking the sum of the columns.  The proportional variance is calculated by dividing the SS_loadings by the number of potential factors (i.e., number of rows).  The cumulative variance is calculated by taking the cumulative sum of the proportional variances.  

```{r}
SS_loadings<-colSums(loadings*loadings)
SS_loadings

Prop_var <- SS_loadings / nrow(loadings)
Prop_var

Cuml_var <-cumsum(Prop_var)
Cuml_var
```

#### Relationship among factors

Looking at the output from `factanal()`, we see that generally there is low correlation between factors except 2 and 4.  High correlation (>0.7) between factors is an indication for redundancy among factors

#### Model fit

At the very bottom of the output, we see the chi-square test for goodness of fit.  The chi-square was `r  as.numeric(agoal.efa$STATISTIC)` with `r  as.numeric(agoal.efa$dof)` df and an associated p-value of `r  agoal.efa$PVAL`.  This value is <0.05 leading us to reject the null hypothesis that the model adequately fits the data and suggesting a four-factor model is not appropriate.  However, given chi-square tests sensitivity to sample size and stringent null hypothesis of exact model data fit, this test is probably not sufficiently reliable.  Further, the *a priori* information about the latent traits exist on multiple factors. 

### So how many factors is appropriate?

```{r}
psych::fa.parallel(goal_scale %>% select(ags1:ags12) , fa="fa", fm="ml")
```

```{r}
agoal.efa3<-factanal(~ags1+ags2+ags3+ags4+ags5+ags6+ags7+ags8+ags9+ags10+ags11+ ags12, factors=3, rotation="promax", data = goal_scale )

agoal.efa3
```


```{r}
loadings3<-as.data.frame(loadings(agoal.efa3)[,])
loadings3%>% 
  rownames_to_column("indicator") %>% 
  left_join(lt) %>% 
  mutate(indicator = factor(indicator, levels = paste("ags",12:1, sep =""))) %>% 
  gather(factor, value, -indicator, - latent_traits) %>% 
  mutate(value2 = ifelse(value < 0.1, NA,  value))-> loadings3.long 


ggplot(data = loadings3.long) +
  geom_point(aes(x = factor, y = indicator, color = value2, shape = latent_traits), size = 8) +
  scale_colour_gradient(na.value = "white", low = "blue", high = "red") +
  scale_shape_manual(values = c("MAP" = 15, "MAV" = 16, "PAP" = 17, "PAV" = 18)) +
  labs(color = "Loading", shape = "Latent\ntrait") +
  theme_classic()
```

### Fit an EFA models with `psych::fa`

```{r}
agoal.efa.4<-fa(goal_scale %>% select(ags1:ags12) , nfactors=3, residuals=TRUE, rotate="promax",SMC=TRUE, fm="pa")
agoal.efa.4
```

Root mean square error of approximation (RMSEA):  By convention, values of RMSEA < 0.05 are taken to indicate good model fit,
and values between 0.05 and 0.08 are seen as indicative of adequate model fit and > 0.08 =  poor fit. 

the Tucker–Lewis index (TLI):  a model is considered to exhibit good fit is when the values of CFI and TLI are 0.95 or higher



n PCA, the components are actual orthogonal linear combinations that maximize the total variance. In FA, the factors are linear combinations that maximize the shared portion of the variance--underlying "latent constructs". 


## References