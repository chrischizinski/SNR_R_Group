---
title: "Applied Multivariate:  Latent class analysis "
output:
  html_notebook: default
editor_options: 
  chunk_output_type: inline
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 10)
```

```{r message = FALSE}
library(poLCA) # poLCA: Polytomous Variable Latent Class Analysis
#library(lavaan) # Latent Variable Analysis
#library(lcca) # Latent Class Causal Analysis
#library(BayesLCA) Bayesian Latent Class Analysis
library(gridExtra)
library(tidyverse)
```

# Latent variable analysis

## Latent class analysis

- Latent class analysis is another technique that is used to describe the latent groups in multivariate data
- Used on polytomous data (so good for analysis of survey data)
- Latent Class Analysis (LCA) is a statistical method for identifying unmeasured class membership among subjects using categorical and/or continuous observed variables. 

- LCA can be used in many disciplines such as Health Sciences, Psychology, Education, and the Social Sciences. 
- Similar to partition clustering and factor analysis, we define the number of groups *a priori*
    - models with successively larger numbers of classes are fitted, and the model that has the fewest classes while still accounting for associations among the observed indicator variables is chosen
    - Unlike FA, which groups along a number of axes, LCA groups people into latent classes.
- It is assumed that individuals can only be in one class
- Each  latent class is characterized by different probabilities of having each of the characteristics related to the condition of interest. 

- The latent class model seeks to stratify the cross-classification table of observed (or, “manifest”) variables by an unobserved (“latent”) unordered categorical variable that eliminates all confounding between the manifest variables.
- “latent class regression” (LCR) -  the probability of latent class membership is predicted by one or more covariates

### Load the data

We will use the data set `election`. The data consisted of two sets of six questions with four responses each, asking respondents’ opinions of
how well various traits describe presidential candidates Al Gore and George W. Bush. Also potential covariates vote choice, age, education, gender, and party ID. 

```{r}
data(election)
head(election)
```

Lets take a quick look at the data.

Challenge
1. Create a data frame of the counts of people that rated each president among Extremely Well:Not Well at all and NAs for each characteristic
2. Create a histogram that displays those counts for each rating for each characteristic for each president

```{r fig.width = 6}

election %>% 
  select(MORALG:INTELG) %>% 
  gather(type, response, MORALG:INTELG) %>% 
  mutate(president = "Gore") -> gore_data

election %>% 
  select(MORALB:INTELB) %>% 
  gather(type, response, MORALB:INTELB) %>% 
  mutate(president = "Bush") %>% 
  rbind(gore_data) %>% 
  mutate(type = gsub("G", "",type),
         type = gsub("B", "",type)) %>% 
  group_by(president, type,response) %>% 
  summarise(N = n()) %>% 
  ggplot() +
  geom_col(aes(x = response, y = N, fill = type), color = "black") +
  facet_grid(president~type) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = "none")

```


If the number of groups fits the data well, the estimated conditional
membership probabilities should be close to 1 or 0; individuals should be
assigned to one of the groups with high probability

### Specifying and using the model

```{r fig.width = 10, fig.height=6}
# basic latent class specification
f.1 <- cbind(MORALG, CARESG, KNOWG, LEADG, DISHONG, INTELG, MORALB, CARESB, KNOWB, LEADB, DISHONB, INTELB) ~ 1

nes.basic <- poLCA(f.1, election, nclass = 3, verbose = FALSE)
nes.basic

names(nes.basic)

as.data.frame(nes.basic$probs) %>% 
  rownames_to_column("class") %>% 
  gather(type, value,-class) %>% 
  separate(type, c("type2", "response", "response2"), sep = "[.]", extra = "merge", fill = "right") %>% 
  mutate(pres = ifelse(substr(type2,nchar(type2),nchar(type2))=="G", "gore", "bush"),
         type2 = substr(type2,1,nchar(type2)-1),
         class = as.numeric(substr(class,nchar(class)-2,nchar(class)-2)) ,
         response2 = factor(as.character(response2), levels = c("Not.too.well","Not.well.at.all","Quite.well", "Extremely.well")))->class_probs

ggplot(class_probs) + 
  geom_col(aes(x = response2, y = value, fill = pres), colour = "black", position = "dodge") + 
  facet_grid(type2~class) +
  theme_classic()

```

How do we interpret these groups?

### Predicted cell frequencies from the latent class model

```{r}

levels(election$MORALB)
out_table <- poLCA.table(formula = MORALG ~ 1, condition = list(MORALB = 1), lc = nes.basic)
 
obs<-as.data.frame(as.matrix(table(election$MORALG[election$MORALB== "1 Extremely well"])))
names(obs) <- "obs"
obs$pred <- c(out_table)
obs
 
```

### Observation classification

```{r}
head(nes.basic$predclass)

```


### Entropy (and other assessments of model fit)

Entropy is a measure of dispersion (or concentration) in a probability mass function.

```{r}
# Log likelihood
 nes.basic$llik

# AIC
 nes.basic$aic
 
# BIC
 nes.basic$bic
 
#Entropy
 
poLCA.entropy(nes.basic)
?poLCA.entropy
```

### Reordering the latent classes
The resultant latent classes are unordered categories, the numerical order of the estimated latent classes in the model output is arbitrary, and is determined solely by the start values of the EM algorithm

repeated runs of `poLCA` will typically produce results containing the same parameter estimates (corresponding to the same maximum log-likelihood), but may have reordered latent class labels

```{r}
nes.basic2 <- poLCA(f.1, election, nclass = 3, verbose = TRUE)

```


### Avoiding local maxima

A well-known drawback of the EM algorithm is that depending upon the initial parameter values chosen in the first iteration, the algorithm may only find a local, rather than the global, maximum of the log-likelihood function

If this does happen, you can reorder your class "names"

```{r}
nes.basic2 <- poLCA(f.1, election, nclass = 3, verbose = TRUE, nrep = 10)

probs.start <- nes.basic2$probs.start

new.probs.start <- poLCA.reorder(probs.start, c(2, 3, 1))

nes.basic3 <- poLCA(f.1, election, nclass = 3, probs.start = new.probs.start)

```

## Latent class regression specification 

The latent class regression model further enables the researcher to estimate the effects of covariates on predicting latent class membership.  Covariates are included in the latent class regression model through their effects on the priors . In the basic latent class model, it is assumed that every individual has the same prior probabilities of latent class membership. The latent class regression model, in contrast, allows individuals' priors to vary depending upon their observed covariates.

We might expect that the way that respondents rate the presidents characteristics will depend on party affiliation.   That is democratic supporters will likely be more favorable to Gore and republican supporters will likely be more favorable to Bust. 

In this data set, `PARTY`  is coded across seven categories, from strong Democrat at 1 to strong Republican at 7. People who primarily consider themselves Independents are at 3-4-5 on the scale.


```{r}
table(election$PARTY)

# latent class regression specification
f.party <- cbind(MORALG, CARESG, KNOWG, LEADG, DISHONG, INTELG, MORALB, CARESB, KNOWB, LEADB, DISHONB, INTELB) ~ PARTY

nes.party <- poLCA(f.party, election, nclass = 3, verbose = FALSE)
nes.party

# Size of each latent class
nes.party$P

```

### Model coefficients and predictions

The neutral group is the first latent class, the favor-Bush group is the second latent class, and the favor-Gore group is the third latent class.

The log-ratio prior probability that a respondent will belong to the favor-Bush group with respect to the neutral group is ln(p2i/p1i) = −3.82 + 0.79 × PARTY

```{r}
nes.party$coeff

pidmat <- cbind(1, c(1:7))
exb <- exp(pidmat %*% nes.party$coeff)
party_pred.num<-data.frame(party = 1:7, pred_prob = (cbind(1, exb)/(1 + rowSums(exb))))

party_pred.num %>% 
  gather(class, value, - party) %>% 
  mutate(class = case_when(class == "pred_prob.1" ~ "Class 1",
                           class == "pred_prob.2" ~ "Class 2",
                           class == "pred_prob.3" ~ "Class 3")) -> party_long

ggplot(data = party_long) + 
  geom_col(aes(x = as.character(party), y = value, fill = class), position = "dodge", colour = "black") +
  labs(y = "Predicted probability", x = "Party affinity", fill = "Latent\nclass") +
  theme_classic() +
  coord_cartesian(ylim = c(0,1), xlim = c(0.5,7.5), expand = FALSE) +
  theme(legend.position = c(0.5, 0.85))

```

### How many classes is correct?

```{r}

set.seed(12348)
classes <- 10


# apply(election.rev,c(2), function(x) any(is.na(x)))
# election.rev<-na.omit(election[c(1:12,17)])

fit.stor <- data.frame(matrix(NA, classes-1,4))

names(fit.stor) <- c("classes", "llik", "aic", "bic")


for(i in 2:classes){
  print(i)
  foo<-poLCA(f.party, election, nclass = i,nrep = 5,verbose = FALSE)
  fit.stor$classes[i - 1] <- i
  fit.stor$llik[i - 1] <- foo$llik
  fit.stor$aic[i - 1] <- foo$aic
  fit.stor$bic[i - 1] <- foo$bic
  # fit.stor$E[i - 1] <- poLCA.entropy(foo)
  
}



fit.stor<-fit.stor[order(fit.stor$bic),]
fit.stor$delta_bic <- fit.stor$bic- fit.stor$bic[1]

fit.stor
```

### Best model

```{r}

f.party <- cbind(MORALG, CARESG, KNOWG, LEADG, DISHONG, INTELG, MORALB, CARESB, KNOWB, LEADB, DISHONB, INTELB) ~ PARTY
out_party<-poLCA(f.party, election.rev, nclass = 5,nrep = 10, verbose = TRUE)


```


```{r}
out_party$coeff

pidmat <- cbind(1, c(1:7))
exb <- exp(pidmat %*% out_party$coeff)
party_pred.num2<-data.frame(party = 1:7, pred_prob = (cbind(1, exb)/(1 + rowSums(exb))))

party_pred.num2 %>% 
  gather(class, value, - party) %>% 
  mutate(class = case_when(class == "pred_prob.1" ~ "Class 1",
                           class == "pred_prob.2" ~ "Class 2",
                           class == "pred_prob.3" ~ "Class 3",
                           class == "pred_prob.4" ~ "Class 4",
                           class == "pred_prob.5" ~ "Class 5")) -> party2_long

ggplot(data = party2_long) + 
  geom_col(aes(x = as.character(party), y = value, fill = class), position = "dodge", colour = "black") +
  labs(y = "Predicted probability", x = "Party affinity", fill = "Latent\nclass") +
  theme_classic() +
  coord_cartesian(ylim = c(0,1), xlim = c(0.5,7.5), expand = FALSE) +
  facet_wrap(~class, ncol = 1) +
  theme(legend.position ="none")
```


## Bayesian Approach

We will use the `BayesLCA` package to explore LCAs using the Bayesian approach.  The data we will use is the `Alzheimer` data set.  This includes the presence or absence of 6 symptoms of Alzheimer's disease (AD) in 240 patients diagnosed with early onset AD conducted in the Mercer Institute in St. James's Hospital, Dublin.

```{r}
data("Alzheimer")
head(Alzheimer)

names(Alzheimer)
# "Hallucination","Activity","Aggression" ,"Agitation","Diurnal","Affective"    

Alzheimer %>% 
  unite(combo, Hallucination:Affective, sep = "") %>% 
  group_by(combo) %>% 
  summarize(N = n()) %>% 
  arrange(desc(N))
```

Lets start to find some classes. First convert the data into the appropriate data set up.

```{r}
alz <- data.blca(Alzheimer)
alz
```

```{r}
alz.3.em <- blca.em(alz, 3, restarts = 20, sd = TRUE)
alz.3.em
```

```{r}
alz3.boot <- blca.boot(alz, fit = alz.3.em, B = 1000, relabel = TRUE)
names(alz3.boot)

as.tibble(alz3.boot$samples$itemprob) %>% 
  gather(all, value) %>% 
  separate(all, c("class","symptom"), "[.]")  -> prob_data_for_plot
  
  
  ggplot(prob_data_for_plot,aes(value,fill = class, colour = class)) +
    geom_density(alpha = 0.35) +
    facet_wrap(~symptom, scales = "free", ncol = 2)  +
    theme_classic()

```

Note the spikes in the plots for the Affective, Diurnal and Aggression symptoms.
This indicates that the parameter estimates in question remained unchanged over all bootstrap
samples.