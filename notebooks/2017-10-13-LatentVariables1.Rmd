---
title: "Latent variable analysis"
csl: the-journal-of-wildlife-management.csl
output:
  html_notebook: default
bibliography: bibliography.bib
editor_options: 
  chunk_output_type: inline
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message = FALSE}
library(psych)
```

## Answers to the challenge

```{r}
library(tidyverse)
library(ggdendro)
library(NbClust)

data("USArrests")

USArrests %>% 
  scale() -> arrest.scale

arrest.scale %>% 
  dist(method = "euclidean") -> arrest.euc

arrest_hclust <- hclust(arrest.euc, method = "ward.D2")

### Find the best number of clusters
arrest.nb <- NbClust(arrest.scale, 
        distance = "euclidean", 
        min.nc = 2, 
        max.nc = 10, 
        method = "complete", 
        index = "all")

clusters <- data.frame(arrest.nb$Best.partition)
clusters$state <- rownames(clusters)
colnames(clusters) <- c("cluster", "label")

### extract the data
arrest_dendro <- as.dendrogram(arrest_hclust)

### convert data to dendro data for ggplot
arrest_dendro_data <- dendro_data(arrest_dendro, type = "rectangle")

### combine cluster and dendro label data 
arrest_dendro_labels <- data.frame(full_join(arrest_dendro_data$labels, clusters))

arrest_dendro_segs <- data.frame(arrest_dendro_data$segments) 
arrest_dendro_segs$cluster <- ifelse(arrest_dendro_segs$x < 20 & arrest_dendro_segs$xend < 20, 1, 2)

### plot 
arrest_dendro_plot<-ggplot() + 
                      geom_segment(data = arrest_dendro_segs, aes(x = x, y = y, xend = xend, yend = yend, color = as.factor(cluster))) + 
                      geom_text(data = arrest_dendro_labels, aes(x = x, y = y, label = label, color = as.factor(cluster)), angle = 90, hjust = 1, size = 1.7, show.legend = FALSE) +
                      labs(color="Cluster") +  
                      coord_cartesian(ylim = c(-5,15)) +
                      theme_void() + 
                      scale_colour_manual(values = c("1" = "firebrick", "2" = "dodgerblue")) +
                      theme(plot.margin = margin(0.5, 0.5, 1, 0.5, unit = "cm")
                      )


arrest_dendro_plot
```

## A quick word about eigenvectors and values

[Eigenvectors and Eigenvalues](http://setosa.io/ev/eigenvectors-and-eigenvalues/)

For any number set of points, we can digest these points into eigenvectors and  eigenvalue.  Eigenvectors and eigenvalues exist in pairs with eigenvector describes a direction along which a linear transformation acts simply by "stretching/compressing" and/or "flipping"; and eigenvalue describing the degree of that transformation in that direction.  The numbers of eigenvectors is equal to the number of dimensions of your data.  

```{r}
mat<-matrix(c(3,2,1,2), byrow = TRUE, ncol = 2)

eigen(mat)
```

# Latent variable analysis

## Factor analysis


The source to many of the notes in this lesson (and a lot more detail on the subject) can be found at @finch2015latent and @beaujean2014latent.  

*Latent variables* in statistics are variables that are not directly observable and are inferred from a mathematical model.  One advantage of using *latent variables* is that it helps reduce the dimensionality of data (a major theme of multivariate statistics) and has been used in many scientific disciplines.  

One type of latent variable analysis is *factor analysis* and used extensively in social and behavioral sciences.  Factor analysis allows the researcher to create models of non-observable factors (e.g., motivations, constraints, identity) from multivariate data.  

There are two broad types of factor analysis:  1) Exploratory factor analysis (EFA) and 2) Confirmatory factor analysis (CFA).  The difference between the two is in the degree of ** a priori ** structure that is assummed in the model.  In using EFA the researcher does not impose a specific latent structure on the observable data, but allows the analysis to provide the optimal number of factors. In contrast to EFA, with CFA the researcher explicitly links the indicators with the factors to which they theoretically belong.

## Exploratory factor analysis

EFA consists of two steps (1) factor extraction and (2) factor rotation.  Factor rotation involves estimating the intial model paramters (i.e., factor loadings:  loadings reflect the
relationships between the factors and the indicators, with larger values being indicative
of a closer association between a latent and observed variable).  There are as many factors as number of indicator variables (i.e., columns used to define the latent variable). 

### Factor extraction

Several extraction methods exist, with the most popular being maximum likelihood (ML) and principal axis factor (PAF).  ML method has a direct assessment of model fit but also relies on multivariate normality.  PAF does not have a distributional assumption but does not have a test of statistical fit.  

### Factor rotation

Factor rotation  is the transformation of the initial set of factor loadings to simplify the interpretation of of the results by finding a simple solution.  Methods fall into two broad categories:  orthogonal and oblique.  Orthogonal rotations constrain the correlations among factors to be zero, whereas oblique rotations allow the factors to be correlated. The most
popular orthogonal rotation method is VARIMAX, while among the oblique rotations PROMAX and OBLIMIN are popular.  Decision on which method to use, should be based in theory and empirical grounds.  

### Determining the optimal number of factors

1. Eigenvalues greater than 1.  Eigenvalues greater than 1, account for more variance than any of the observed values.  
2.  Scree plots.  Eigenvalues on the y-axis and number of factors on the x axis.  Identify where the plot "flattens out"
3. Proportion of variance.  Identify when there is no "appreciative"  gain in the percentage of variance explained.  Similar to scree plot
4. Residual correlation matrix for the observed indicators. residuals larger than 0.05 are considered to be too large, so that a good solution is one which produces few residual correlations greater than 0.05 in absolute value.
5. Parallel analysis.  Described in @horn1965rationale.  Several steps that include comparing the actual data to  two randomly generated datasets.  THis is the most commonly used. 

### The data 

For this data, we will use the data set provided by @finch2015latent [here](https://www.routledge.com/Latent-Variable-Modeling-with-R/Finch-French/p/book/9780415832458#datasets).  The data represents information collected on acheivement goal orientation. Achievement goal orientation refers to how an individual interprets and reacts to tasks, resulting in different patterns of cognition, affect and behavior.  There are 12 questions with results representing a 7-point likert-type scale from 430 college students. 

The columns refer to:

- AGS1 = My goal is to completely master the material presented in my classes. (MAP)
- AGS2 = I want to avoid learning less than it is possible to learn. (MAV)
- AGS3 = It is important for me to do better than other students. (PAP)
- AGS4 = I want to avoid performing poorly compared to others. (PAV)
- AGS5 = I want to learn as much as possible. (MAP)
- AGS6 = It is important for me to avoid an incomplete understanding of the course material. (MAV) 
- AGS7 = It is important for me to understand the content of my courses as thoroughly as possible. (MAP)
- AGS8 = My goal is to avoid performing worse than other students. (PAV) 
- AGS9 = I want to do well compared to other students. (PAP) 
- AGS10 = It is important for me to avoid doing poorly compared to other students. (PAV) 
- AGS11 = My goal is to perform better than the other students. (PAP) 
- AGS12 = My goal is to avoid learning less than I possibly could. (MAV)


The types of questions refer to 4 distinct latent traits: mastery approach (MAP), mastery avoidant (MAV), performance approach (PAP), and performance avoidant (PAV).

The data is in a SPSS format and I have converted it to a csv file for convience and is in our github data repository as `goal_scale.csv`

```{r}
library(readr)

goal_scale <- read_csv("https://raw.githubusercontent.com/chrischizinski/SNR_R_Group/master/data/goal_scale.csv")

head(goal_scale)

```


### Fit an EFA models with `factanal`

```{r}

agoal.efa<-factanal(~ags1+ags2+ags3+ags4+ags5+ags6+ags7+ags8+ags9+ags10+ags11+ ags12, factors=4, rotation="promax", data = goal_scale )

agoal.efa

```

#### Uniqueness 

Uniqueness reflects the proportion of variance in the indicators that are not explained by the factors.  For example, `r round(agoal.efa$uniquenesses[1], digits = 3)*100 `% of variation in `ags1` is not explained by the four factors. 


```{r}
agoal.efa$uniquenesses
```

The opposite of uniqueness of communality, and this is the proportion of variances explained by the factors for each indicator.

```{r}
1-agoal.efa$uniquenesses
```


#### Loadings

```{r}
ld<-loadings(agoal.efa)
ld

```
To help interpret our loadings, lets create a visualization of those loadings.  

```{r}
loadings<-as.data.frame(ld[,])

lt<- data.frame(indicator = paste("ags",1:12, sep =""),
           latent_traits = c("MAP", "MAV", "PAP", "PAV", "MAP","MAV", "MAP", "PAV", "PAP", "PAV", "PAP", "MAV"))

loadings %>% 
  rownames_to_column("indicator") %>% 
  left_join(lt) %>% 
  mutate(indicator = factor(indicator, levels = paste("ags",12:1, sep =""))) %>% 
  gather(factor, value, -indicator, - latent_traits) %>% 
  mutate(value2 = ifelse(value < 0.1, NA,  value))-> loadings.long 


ggplot(data = loadings.long) +
  geom_point(aes(x = factor, y = indicator, color = value2, shape = latent_traits), size = 8) +
  scale_colour_gradient(na.value = "white", low = "blue", high = "red") +
  scale_shape_manual(values = c("MAP" = 15, "MAV" = 16, "PAP" = 17, "PAV" = 18)) +
  labs(color = "Loading", shape = "Latent\ntrait") +
  theme_classic()
```


## References