geom_errorbar(aes(x = as.numeric(Type), ymax = ymax, ymin = ymin), width = 0.5) +
geom_point(aes(x =  as.numeric(Type), y = M_score), colour="black", size = 4) +
geom_text(data= data_label,aes(x =  as.numeric(Type), y = 100, label = label)) +
scale_colour_brewer(palette = "Set1") +
coord_cartesian(ylim = c(0, 100), xlim = c(-0, 10.1), expand = F) +
labs(y = "Scaled score", x ="") +
theme_bw() +
theme(axis.text.x = element_blank())
ggplot(mean_scores) +
geom_bar(aes(x =  as.numeric(Type), y = M_score), stat = "identity", fill = "dodgerblue", colour="black") +
geom_errorbar(aes(x = as.numeric(Type), ymax = ymax, ymin = ymin), width = 0.5) +
geom_point(aes(x =  as.numeric(Type), y = M_score), colour="black", size = 4) +
geom_text(data= data_label,aes(x =  as.numeric(Type), y = 100, label = label)) +
scale_colour_brewer(palette = "Set1") +
coord_cartesian(ylim = c(0, 100), xlim = c(-0, 11), expand = F) +
labs(y = "Scaled score", x ="") +
theme_bw() +
theme(axis.text.x = element_blank())
ggplot(mean_scores) +
geom_bar(aes(x =  as.numeric(Type), y = M_score), stat = "identity", fill = "dodgerblue", colour="black") +
geom_errorbar(aes(x = as.numeric(Type), ymax = ymax, ymin = ymin), width = 0.5) +
geom_point(aes(x =  as.numeric(Type), y = M_score), colour="black", size = 4) +
geom_text(data= data_label,aes(x =  as.numeric(Type), y = 90, label = label)) +
scale_colour_brewer(palette = "Set1") +
coord_cartesian(ylim = c(0, 100), xlim = c(-0, 11), expand = F) +
labs(y = "Scaled score", x ="") +
theme_bw() +
theme(axis.text.x = element_blank())
citation(package = "ordinal")
citation(package = "ordinal")
install.packages("ordinal")
citation(package = "ordinal")
citation(package = "lme4")
install.package("lme4")
install.packages("lme4")
citation(package = "lme4")
liobrary(lme4)
library(lme4)
?predict.merMod
134876 -40000
58690 /(134876 -40000)
80 - 25
80 - 45
fake
fake <- data.frame(
subj = factor(1:5),
corr = c(60, 65, 45, 61, 59),
wrong = c(20, 15, 35, 19, 21))
fake <- data.frame(
subj = factor("pyruvate", "pyr.proc", "aceteyl", "fadh2", "electron"),
corr = c(60, 65, 45, 61, 59),
wrong = c(20, 15, 35, 19, 21))
fake
fake <- data.frame(
subj = factor(c("pyruvate", "pyr.proc", "aceteyl", "fadh2", "electron")),
corr = c(60, 65, 45, 61, 59),
wrong = c(20, 15, 35, 19, 21))
fake
fake.glm = glm(cbind(pos, neg) ~ subj, family = binomial(), data = fake)
fake.glm = glm(cbind(corr, wrong) ~ subj, family = binomial(), data = fake)
summary(fake.glm)
anova(fake.glm)
?dchisq
pchisq(13.885,4)
pchisq(13.885,4, lower.tail = FALSE)
install.packages("lsmeans")
fake %>%
gather(type, value, corr,wrong)
library(tidyverse)
fake %>%
gather(type, value, corr,wrong)
fake <- data.frame(
subj = factor(c("pyruvate", "pyr.proc", "aceteyl", "fadh2", "electron")),
corr = c(60, 65, 45, 61, 59),
wrong = c(20, 15, 35, 19, 21))
fake %>%
gather(type, value, corr,wrong)
fake <- data.frame(
subj = factor(c("pyruvate", "pyr.proc", "aceteyl", "fadh2", "electron")),
corr = c(60, 65, 45, 61, 59),
wrong = c(20, 15, 35, 19, 21))
fake
fake %>%
gather(type, value, corr,wrong) -> fake2
ggplot(data = fake2) +
geom_bar(aes( x = subj, y = value, fill = type), stat = "identity", colour = "black") +
theme_bw()
ggplot(data = fake2) +
geom_bar(aes( x = subj, y = value, fill = type), stat = "identity", colour = "black") +
scale_fill_manual(values = c("corr" = "blue", "wrong" = "red")) +
theme_bw()
fake <- data.frame(
subj = factor(c("pyruvate", "pyr.proc", "aceteyl", "fadh2", "electron"),
levels =c("pyruvate", "pyr.proc", "aceteyl", "fadh2", "electron") ),
corr = c(60, 65, 45, 61, 59),
wrong = c(20, 15, 35, 19, 21))
fake
fake %>%
gather(type, value, corr,wrong) -> fake2
ggplot(data = fake2) +
geom_bar(aes( x = subj, y = value, fill = type), stat = "identity", colour = "black") +
scale_fill_manual(values = c("corr" = "blue", "wrong" = "red")) +
theme_bw()
library(lsmeans)
( fake.lsm = lsmeans(fake.glm, "subj") )
fake.glm = glm(cbind(corr, wrong) ~ subj, family = binomial(), data = fake)
summary(fake.glm)
anova(fake.glm)
pchisq(13.885,4, lower.tail = FALSE)
( fake.lsm = lsmeans(fake.glm, "subj") )
( fake.lsmp = regrid(fake.lsm, transform = TRUE) )
contrast(fake.lsm, "del.eff")
contrast(fake.lsm, "del.eff",, transform = TRUE)
contrast(fake.lsm, "del.eff", transform = TRUE)
( foo.lsm = lsmeans(foo.gl
( foo.lsm = lsmeans(foo.glm, "subj") )
( foo.lsmp = regrid(foo.lsm, transform = TRUE) )
( foo.lsm = lsmeans(foo.glm, "subj") )
foo.glm = glm(cbind(corr, wrong) ~ subj, family = binomial(), data = foo)
summary(foo.glm)
anova(foo.glm)
foo <- data.frame(
subj = factor(c("pyruvate", "pyr.proc", "aceteyl", "fadh2", "electron"),
levels =c("pyruvate", "pyr.proc", "aceteyl", "fadh2", "electron") ),
corr = c(60, 65, 45, 61, 59),
wrong = c(20, 15, 35, 19, 21))
foo
foo %>%
gather(type, value, corr,wrong) -> foo2
library(tidyverse)
library(lsmeans)
foo <- data.frame(
subj = factor(c("pyruvate", "pyr.proc", "aceteyl", "fadh2", "electron"),
levels =c("pyruvate", "pyr.proc", "aceteyl", "fadh2", "electron") ),
corr = c(60, 65, 45, 61, 59),
wrong = c(20, 15, 35, 19, 21))
foo
foo %>%
gather(type, value, corr,wrong) -> foo2
ggplot(data = foo2) +
geom_bar(aes( x = subj, y = value, fill = type), stat = "identity", colour = "black") +
scale_fill_manual(values = c("corr" = "blue", "wrong" = "red")) +
theme_bw()
foo.glm = glm(cbind(corr, wrong) ~ subj, family = binomial(), data = foo)
summary(foo.glm)
anova(foo.glm)
pchisq(13.885,4, lower.tail = FALSE)
( foo.lsm = lsmeans(foo.glm, "subj") )
contrast(fake.lsm, "del.eff")
contrast(foo.lsm, "del.eff")
?lsmeans
( foo.lsm = lsmeans(foo.glm, "subj", contr ="del.eff") )
( foo.lsm = lsmeans(foo.glm, "subj", contr ="del.eff") )
( foo.lsmp = regrid(foo.lsm, transform = TRUE) )
( foo.lsm = lsmeans(foo.glm, "subj", contr ="del.eff", transform = TRUE) )
install.packages("multcompView")
( foo.lsm = lsmeans(foo.glm, "subj") )
contrast(foo.lsm, "del.eff")
cld(foo.lsm)
( foo.lsm = lsmeans(foo.glm, "subj", contr ="del.eff") )
cld(foo.lsm)
( foo.lsmp = regrid(lsmeans(foo.glm, "subj"), transform = TRUE) )
( foo.lsm = lsmeans(foo.glm, "subj", contr ="pairwise") )
cld(foo.lsm)
( foo.lsm = lsmeans(foo.glm, "subj", contr ="pairwise") )
cld(foo.lsm)
( foo.lsm = lsmeans(foo.glm, "molecule", contr ="pairwise") )
data<- data.frame(type = c("pre","post"),
correct = c(15,22),
total = c(99,99))
mod.out <- glm(cbind(correct,total) ~ type, family = binomial(), data = data)
summary(mod.out)
anova(mod.out)
install.pacakges(c("rPython","rpy","rpy2"))
install.packages(c("rPython","rpy","rpy2"))
install.packages("DCchoice")
install.packages("DCchoice",dependencies = TRUE)
install.packages("Icens")
install.packages("DCchoice",
repos = c("http://www.bioconductor.org/packages/release/bioc",
"https://cran.rstudio.com/"),
dep = TRUE)
install.packages("â€˜support.BWS2")
install.packages("support.BWS2")
library(DCchoice)
data(NaturalPark, package = "Ecdat")
head(NaturalPark)
NaturalPark$R1 <- ifelse(substr(NaturalPark$answers, 1, 1) == "y", 1, 0)
NaturalPark$R2 <- ifelse(substr(NaturalPark$answers, 2, 2) == "y", 1, 0)
NaturalPark$LBD1 <- log(NaturalPark$bid1)
NaturalPark$bid2 <- ifelse(NaturalPark$R1 == 1, NaturalPark$bidh, NaturalPark$bidl)
NaturalPark$LBD2 <- log(NaturalPark$bid2)
fmdb <- R1 + R2 ~ sex + age + income | LBD1 + LBD2
fmdb <- R1 + R2 ~ sex + age + income | log(bid1) + log(bid2)
NPdb <- dbchoice(fmdb, data = NaturalPark)
NPdb
NPdbs <- summary(NPdb)
NPdbs
krCI(NPdb)
bootCI(NPdb)
krCI(NPdb, individual = data.frame(sex = "female", age = 5, income = 3))
bid.design <- unique(NaturalPark[, c(1:3)])
bid.design <- log(bid.design)
colnames(bid.design) <- c("LBD1", "LBDH", "LBDL")
bid.design
head(predict(NPdb, type = "utility", bid = bid.design))
bid.design <- unique(NaturalPark[, c(1:3)])
bid.design
bid.design <- log(bid.design)
colnames(bid.design) <- c("LBD1", "LBDH", "LBDL")
bid.design
head(predict(NPdb, type = "utility", bid = bid.design))
head(predict(NPdb, type = "probability", bid = bid.design))
predict(NPdb, type = "utility",
newdata = data.frame(sex = "female", age = 5, income = 3, LBD1 = log(10)))
install.packages("psych", dependencies = TRUE)
install.packages(c("evd","randtoolbox","mclogit"))
####
#
#	MIXED LOGIT MODEL ESTIMATION IN WPT (or VTT) SPACE
#
#     	Date: August 2014
#
#	This code was developed by Carlo Fezzi.
#
#	This code generates multinomial choice data and then estimate a mixed logit model in WTP space (Train and Weeks, 2005; Fezzi et al., 2014), with gaussian
#	distributed and uncorrelated random parameter. Please cite Fezzi et al. (2014) if you are using this code.
#
#	If you find any mistake or if you have any tips on how to speed up the code (quite slow at the moment) feel free to email: c.fezzi@uea.ac.uk
#
#	In addition the data generation requires a the simulation of random draws from distribution which here I implement
#	by using the "edv" package, written by Alec Stephenson, and the "randtoolbox" package maintained by Christophe Dutang.
#
#	Note: the likelihood of these models is not globally concave so you should try different starting values
#
#	References
#
#	Train, K.E., Weeks, M. (2005) "Discrete choice models in preference space and willing-to-pay space". In: Scarpa, R., Alberini, A. (Eds.), Applications of
#	Simulation Methods in Environmental and Resource Economics, Springer Publisher, Dordrecht, pp. 2?16. (chapter 1).
#
#	Fezzi C., Bateman I.J., Ferrini S. (2014) "Using revealed preferences to estimate the value of travel time to recreation sites", Journal of Environmental Economics and Management, vol. 67, pp. 58?70
#
#####
rm(list=ls())
library(evd)
library(randtoolbox)
library(mclogit)
### FUNCTIONS ####
### max.id = function which calculates which option in chosen by each respondent based on the expected utility
max.id <- function(X, id)
{
maxi <- data.frame(massi=tapply(X, id, max),id=unique(id))
id <- data.frame(id=id)
mm <- merge(id,maxi)
out <- numeric(length(X))
out[X == mm$massi] <- 1
return(out)
}
################################# DATA GENERATION #############################
#													#
# generate the simulated data on which we then estimate the mixed logit model #
#													#
###############################################################################
set.seed(333)
OBS <- 400		## number of respondents
N <- OBS
choices <- 3	## number of choice options in each choice
sets <- 10		## number of choices
NIV <- 2		## number of parameters
#############################
id <- rep(1:OBS, each=sets*choices)		# person id
idc <- rep(1:(OBS*sets), each=choices)	# choice id
####-->### NOTE: CHANGE HERE, ADDING OR REDUCING VARIABLES IF YOU SET NIV DIFFERENT FROM 2 ###
### generate explanatory variables ###
X1 <- runif(OBS*choices*sets, min=1, max=5)
X2 <- runif(OBS*choices*sets, min=1, max=5)
X <- cbind(X1,X2)
LAMBDA1 <- rnorm(OBS, mean=-2, sd=0.5)
LAMBDA2 <- rnorm(OBS, mean=0.9, sd=0.3)
hist(LAMBDA1)
hist(LAMBDA2)
LAMBDA1 <- rep(LAMBDA1, each=sets*choices)
LAMBDA2 <- rep(LAMBDA2, each=sets*choices)
e <- rgumbel(OBS*choices*sets, loc=0, scale=1)
U <- LAMBDA1 * ( X1 + LAMBDA2 * X2) + e
####-->################################################
### simulate choices ###
ch <- max.id(U, idc)
########################
####-->###  also add the new Xs to the dataframe if you are setting NIV different from 2
dati <- data.frame(ch,idc,id, X1, X2)
###########################
###### HALTON DRAWS #######
###########################
K <- 10  				# number of halton draws per person
draws <- halton(N * K + 10, normal = TRUE, init=TRUE, dim= NIV)	# generating the draws (discards the first 10 draws)
### --> ## also change here if you have more than 2 random parameters in your model
draws1 <- draws[-c(1:10),1]
draws2 <- draws[-c(1:10),2]
draws1 <- matrix(draws1, N, K, byrow=TRUE)
draws2 <- matrix(draws2, N, K, byrow=TRUE)
### --> ##
beta <- c(-2,0.2,0.8,0.3)
###########################################
######### LOG LIKELIHOOD FUNCTION #########
###########################################
##############
#### START ###
##############
newL <- function(beta = numeric(2))
{
P <- matrix(0, OBS, 1)			# final probabilities
person <- 0
for( i in unique(id))			# loop over respondents
{
person <- person + 1
#### change here if NIV different from 2... one draws for each paramter ###
draws1.i <- draws1[person,]						# draws for respondent i, parameter of X1
draws2.i <- draws2[person,]						# draws for respondent i, parameter of X2
##############################################
X.i <- X[id==i,]	### Xs belonging to person i
ch.i <- ch[id==i]	### choices of person i
idc.i <- idc[id==i]	### choice id of person i
NC.i <- length(unique(idc.i))	 ## number of choices of person i (in case you have a different number of choices per person)
Lik <- matrix(0, K, 1)		## probabily for each draw for person i
for(k in 1:K)				## loop over draws
{
draw1.k <- draws1.i[k]					# draw k for respondent i, parameter of X1
draw2.k <- draws2.i[k]					# draw k for respondent i, parameter of X2
beta1.k <- ( draw1.k * beta[2] ) + beta[1]			### betas for person i
beta2.k <- ( draw2.k * beta[4] ) + beta[3]			### betas for person i
beta.k <- cbind(beta1.k, beta2.k)			# all betas together, change here if NIV dif from 2
Psets <- matrix(0, NC.i , 1)		# probability of each of the choices for person i, given draw k
choice <- 0
for (j in unique(idc.i) )		# loop over choices
{
choice <- choice + 1				## logit model
Xj <- X.i[idc.i==j , ]
chj <- ch.i[idc.i==j]
#### UTILITY EQUATION HERE #####  change here if you use a different equation
beta1 <- beta.k[1,1]
beta2 <- beta.k[1,2]
Xj1 <- Xj[,1]
Xj2 <- Xj[,2]
eqj <- beta1 * (Xj1  + beta2 * Xj2)
################################
numj <- exp(eqj)
denj <- sum(numj)
pj <- numj[chj==1] / denj
Psets[choice] <- pj
}
Lik[k] <- prod(Psets)			## probability of the all the choices given the draw for person i
}
P[person] <- mean(Lik)				## mean probability given the K draws
}
return(-sum(log(P)))
}
###########
### END ###
###########
#### estimation #####
### A) compute starting values via a standard logit model using "mclogit"
start <-  mclogit( cbind(ch,id) ~  X1  + X2)
### B) estimate the logit model (quite slow)
result <- optim(par = c(coef(start)[1],0.5 , coef(start)[2]/coef(start)[1], 0.5 ) , newL, hessian = TRUE,
method = "Nelder-Mead", control=list(trace=1) )
result
sqrt(diag(solve(result$hessian)))		# standard errors
library(glmmADMB)
install.packages("R2admb")
install.packages("glmmADMB",
repos=c("http://glmmadmb.r-forge.r-project.org/repos",
getOption("repos")),
type="source")
library(glmmADMB)
?glmmadmb
2.4*4
9.6/52
0.185 * 4
0.74*5
400/40
10/52
(10/52)*4
((10/52)*4)*3
install.packages("odbc")
install.packages("odbc", dependencies = TRUE)
library(odbc)
install.packages(c("DBI","config"))
install.packages(c("DBI", "config"))
library(DBI)
library(config)
install.packages("dbplyr")
install.packages("keyring")
shiny::runGadget(sparklyr::connection_spark_shinyapp(), viewer = .rs.embeddedViewer)
shiny::runApp('Documents/NGPC_Dashboard_Portable/app/shiny')
runApp('Documents/NGPC_Dashboard_Portable/app/shiny')
shiny::runApp('Documents/NGPC_Dashboard_Portable/app/shiny')
install.packages(c("BH", "boot", "curl", "desc", "fda", "foreign", "haven", "httr", "knitr", "Matrix", "memisc", "mgcv", "modelr", "purrr", "Rcpp", "rlang", "sandwich", "scales", "shiny", "sparklyr", "tibble", "tidyr"))
shiny::runApp('Documents/NGPC_Dashboard_Portable/app/shiny')
install.packages("tidyverse")
install.packages("tidyverse")
shiny::runApp('Documents/NGPC_Dashboard_Portable/app/shiny')
?syms
?UQS
install.packages("rlang")
install.packages("rlang")
shiny::runApp('Documents/NGPC_Dashboard_Portable/app/shiny')
source('plotSwitch.R', local = TRUE)
setwd("/Users/cchizinski2/Documents/NGPC_Dashboard_Portable")
source('plotSwitch.R', local = TRUE)
setwd("/Users/cchizinski2/Documents/NGPC_Dashboard_Portable/shiny")
setwd("/Users/cchizinski2/Documents/NGPC_Dashboard_Portable/app/shiny")
source('plotSwitch.R', local = TRUE)
source('timeSeries.R', local = TRUE)
source('treemap.R', local = TRUE)
source('ageDist.R', local = TRUE)
source('map.R', local = TRUE)
source('permitGroups.R', local = TRUE)
source('propBar.R', local = TRUE)
source("define_ggplot_presentation_theme.R", local = TRUE)
dateRange <-
data_frame(minDate = "2007-11-05", maxDate = "2017-05-05")
myLevels <-
data_frame(permitType = c("Antelope",
"Apprentice Certificate",
"Aquatic Stamp",
"Bighorn Sheep",
"Deer",
"Duplicate Park Entry",
"Elk",
"Fall Turkey",
"Fish",
"Fur Harvest",
"Habitat Stamp",
"Hunt",
"Hunt Fish Combo",
"Hunter Ed Archery",
"Hunter Ed Firearm",
"Mountain Lion",
"Multi-Species",
"Non-Resident BOAT AIS Stamp",
"Paddlefish Archery",
"Paddlefish Snagging",
"Park",
"Spring Turkey",
"Temporary Federal Duck Stamp",
"Waterfowl Stamp")
) %>%
arrange(permitType)
data <- tbl_dt(readRDS("data/sampleData.RDS"))
runApp()
head(data)
localData %>%
filter(permitType %in% c(input$permitTypes, input$combo1, input$combo2, input$combo3)
)
localData <- localData %>%
filter(permitType %in% c(input$permitTypes, input$combo1, input$combo2, input$combo3),
# permitType %in% input$permitTypes,
issueDate >= input$dates[1], issueDate <= input$dates[2],
# lubridate::month(issueDate, abbr = T) %in% input$issueMonths,
age >= input$age[1], age <= input$age[2],
gender %in% input$gender | ("NA" %in% input$gender & is.na(gender)),
permitYear %in% input$permitYears,
# ,
# UATYP10 %in% geoClass,
# lifeRange %in% input$duration,
resident %in% input$residentPermit | ("NA" %in% input$residentPermit & is.na(resident)),
# youth %in% input$youthPermit,
# landowner %in% input$landownerPermit,
# disabled %in% input$disabledPermit,
# veteran %in% input$veteranPermit,
# senior %in% input$seniorPermit,
# deployMilitary %in% input$deployedPermit,
stateResident %in% input$residentLocation | ("NA" %in% input$residentLocation & is.na(stateResident))
# county %in% input$countyNames | "All" %in% input$countyNames,
# city %in% input$cityNames | "All" %in% input$cityNames
)
runApp()
head(data)
data <- data %>%
mutate(gender = factor(gender),
resident = factor(resident),
permitYear = factor(permitYear),
permitType = factor(permitType)) %>%
mutate(year = as.integer(year(issueDate)),
month = lubridate::month(issueDate, label = T, abbr = T),
monthYear = paste(lubridate::month(issueDate, label = T, abbr = T),
substr(year(issueDate), 3, 4), sep = " "),
dayOfYear = yday(issueDate))
data$monthYear <-
factor(data$monthYear,
levels = unique(data$monthYear[order(year(data$issueDate), month(data$issueDate))]))
data <- data %>%
select(customerUID, UQS(syms(groupVars)))
groupVars
syms(groupVars)
UQS(syms(groupVars))
data <- data %>%
select_(customerUID, UQS(syms(groupVars)))
KnitPost <- function(input, base.url = "/SNR_R_Group/") {
require(knitr)
opts_knit$set(base.url = base.url)
fig.path <- paste0("figs/", sub(".Rmd$", "", basename(input)), "/")
opts_chunk$set(fig.path = fig.path)
render_jekyll()
knit(input, envir = parent.frame())
}
setwd("/Users/cchizinski2/Documents/SNR_R_Group/master/Rmd/")
KnitPost("2017-08-25-IntrotoR.Rmd")
KnitPost("2017-08-25-IntrotoR.Rmd")
update.packages()
update.packages()
